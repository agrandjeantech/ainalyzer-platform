# üîÑ Workflow d'Analyse - Ainalyzer Platform

## üìã Vue d'ensemble

Ce document d√©crit le workflow complet d'analyse d'images dans la plateforme Ainalyzer, de l'upload √† l'affichage des r√©sultats avec annotations.

## üëÅÔ∏è Comment Visualiser le Sch√©ma

### **Option 1 : GitHub (Recommand√©)**
- Ouvrez ce fichier directement sur **GitHub** dans votre navigateur
- GitHub affiche automatiquement les diagrammes Mermaid
- URL : `https://github.com/agrandjeantech/ainalyzer-platform/blob/master/ANALYSIS_WORKFLOW.md`

### **Option 2 : VSCode avec Extension**
1. Installez l'extension **"Mermaid Preview"** dans VSCode
2. Ouvrez ce fichier `.md` dans VSCode
3. Utilisez `Ctrl+Shift+P` ‚Üí "Mermaid: Preview"

### **Option 3 : √âditeur Mermaid en Ligne**
1. Copiez le code Mermaid ci-dessous
2. Allez sur **https://mermaid.live/**
3. Collez le code pour voir le diagramme interactif

### **Option 4 : Notion, Obsidian, etc.**
- La plupart des √©diteurs Markdown modernes supportent Mermaid
- Copiez-collez ce fichier dans votre √©diteur pr√©f√©r√©

## üéØ Sch√©ma du Workflow

```mermaid
graph TD
    A[üë§ Utilisateur] --> B[üì± Page /real-analyze]
    B --> C{üîê Authentifi√© ?}
    C -->|Non| D[üö™ Redirection /login]
    C -->|Oui| E[üîë V√©rification cl√©s API]
    
    E --> F{üóùÔ∏è Cl√©s API configur√©es ?}
    F -->|Non| G[‚ö†Ô∏è Message: Configurez vos cl√©s API]
    F -->|Oui| H[üì§ Interface d'upload]
    
    H --> I[üìÅ S√©lection fichier image]
    I --> J[üîç Validation fichier]
    J --> K{‚úÖ Fichier valide ?}
    K -->|Non| L[‚ùå Erreur: Format/taille invalide]
    K -->|Oui| M[‚òÅÔ∏è Upload vers Supabase Storage]
    
    M --> N[üíæ Enregistrement en base]
    N --> O[üéØ S√©lection types d'analyse]
    O --> P[ü§ñ S√©lection provider IA]
    P --> Q[‚ñ∂Ô∏è Lancement analyse]
    
    Q --> R[üîÑ Boucle pour chaque type]
    R --> S[üì° Appel API /analyze]
    S --> T[üîê V√©rification auth + cl√©s]
    T --> U[üñºÔ∏è R√©cup√©ration image]
    U --> V[üìù R√©cup√©ration prompt]
    V --> W{ü§ñ Provider ?}
    
    W -->|OpenAI| X[üü¢ Analyse GPT-4o]
    W -->|Anthropic| Y[üü£ Analyse Claude 3.5]
    
    X --> Z[üìä Traitement r√©ponse]
    Y --> Z
    Z --> AA[üíæ Sauvegarde r√©sultats]
    AA --> BB[üì§ Retour r√©sultats]
    
    BB --> CC{üîÑ Autres analyses ?}
    CC -->|Oui| R
    CC -->|Non| DD[üé® Affichage annotations]
    
    DD --> EE[üëÅÔ∏è Visualisation interactive]
    EE --> FF[üí¨ Chat IA disponible]
    FF --> GG[üîÑ Possibilit√© nouvelles analyses]
```

## üìä D√©tail des √âtapes

### 1. üö™ **Authentification & V√©rifications**

```typescript
// V√©rification utilisateur
const { data: { user } } = await supabase.auth.getUser()
if (!user) redirect('/login')

// V√©rification cl√©s API
const apiKeys = await getApiKeys(user.id)
if (!apiKeys.length) showApiKeyWarning()
```

### 2. üì§ **Upload d'Image**

```typescript
// Validation fichier
const validation = validateImageFile(file)
// Types support√©s: JPG, PNG, WebP (max 10MB)

// Upload Supabase Storage
const { data } = await supabase.storage
  .from('analysis-images')
  .upload(filePath, file)

// Enregistrement m√©tadonn√©es
const imageRecord = await supabase
  .from('images')
  .insert({
    user_id: user.id,
    original_name: file.name,
    storage_path: filePath,
    file_hash: generateHash(file),
    // ...autres m√©tadonn√©es
  })
```

### 3. üéØ **Configuration d'Analyse**

```typescript
// S√©lection types d'analyse
const selectedTypes = ['accessibility', 'ui-ux', 'technical']

// S√©lection provider IA
const selectedProvider = 'anthropic' // ou 'openai'

// Synchronisation entre composants
<AnalysisChat 
  selectedProvider={selectedProvider}
  onProviderChange={setSelectedProvider}
  // ...autres props
/>
```

### 4. ü§ñ **Traitement IA**

#### **üß© Sch√©ma D√©taill√© - Construction des Prompts**

```mermaid
graph TD
    A[üì° API /analyze] --> B[üîç R√©cup√©ration AnalysisType]
    B --> C[üìã Donn√©es de base r√©cup√©r√©es]
    
    C --> D[üìù system_prompt]
    C --> E[ü§ù coordination_prompt]
    C --> F[üé® formatting_instructions]
    C --> G[üìç annotation_rules]
    
    H[üîß buildEnhancedPrompt] --> I[üìù Prompt de base]
    D --> I
    
    I --> J{ü§ù Coordination ?}
    J -->|Oui| K[‚ûï Ajout coordination_prompt]
    J -->|Non| L[‚è≠Ô∏è √âtape suivante]
    K --> L
    E --> K
    
    L --> M[üé® Ajout instructions formatage]
    F --> N[üìã Instructions par d√©faut]
    F --> O{üé® Custom formatting ?}
    O -->|Oui| P[üìã Instructions personnalis√©es]
    O -->|Non| N
    P --> M
    N --> M
    
    M --> Q[üìç Ajout r√®gles annotations]
    G --> R[üìç R√®gles par d√©faut]
    G --> S{üìç Custom rules ?}
    S -->|Oui| T[üìç R√®gles personnalis√©es]
    S -->|Non| R
    T --> Q
    R --> Q
    
    Q --> U[üîó Assemblage final]
    U --> V[‚úÖ Prompt enrichi complet]
    
    V --> W{ü§ñ Provider ?}
    W -->|OpenAI| X[üü¢ GPT-4o + Prompt]
    W -->|Anthropic| Y[üü£ Claude 3.5 + Prompt]
    
    X --> Z[üìä R√©ponse structur√©e]
    Y --> Z
    
    style D fill:#e1f5fe
    style E fill:#f3e5f5
    style F fill:#e8f5e8
    style G fill:#fff3e0
    style V fill:#ffebee
```

#### **Route API `/api/analyze`**

```typescript
// Pour chaque type d'analyse s√©lectionn√©
for (const analysisTypeId of selectedAnalysisTypes) {
  
  // 1. R√©cup√©ration du prompt syst√®me
  const analysisType = await getAnalysisType(analysisTypeId)
  
  // 2. Construction du prompt enrichi
  const enhancedPrompt = buildEnhancedPrompt(analysisType)
  
  // 3. Appel IA selon le provider
  if (provider === 'openai') {
    result = await analyzeWithOpenAI(apiKey, imageUrl, analysisType)
  } else if (provider === 'anthropic') {
    result = await analyzeWithAnthropic(apiKey, imageUrl, analysisType)
  }
  
  // 4. Sauvegarde r√©sultats
  await supabase.from('analyses').insert({
    user_id: user.id,
    image_id: imageId,
    analysis_type_id: analysisTypeId,
    provider: provider,
    result_json: result,
    status: 'completed'
  })
}
```

#### **üîß Fonction buildEnhancedPrompt - D√©tail**

```typescript
function buildEnhancedPrompt(analysisType: AnalysisType) {
  // 1. üìù Prompt de base (m√©tier)
  let enhancedPrompt = analysisType.system_prompt

  // 2. ü§ù Ajout coordination si disponible
  if (analysisType.coordination_prompt) {
    enhancedPrompt += `\n\nINSTRUCTIONS DE COORDINATION SP√âCIFIQUES:\n${analysisType.coordination_prompt}`
  }

  // 3. üé® Instructions de formatage
  const defaultFormatting = "R√¥le et fonction : [description] | Position et hi√©rarchie : [description] | D√©limitation visuelle : [description] | Probl√®mes d'accessibilit√© : [liste] | Suggestions de code : [code HTML/CSS]"
  const formattingInstructions = analysisType.formatting_instructions || defaultFormatting

  // 4. üìç R√®gles d'annotation
  const defaultRules = "Coordonn√©es pr√©cises en pourcentages de l'image. x,y = coin sup√©rieur gauche de la zone."
  const annotationRules = analysisType.annotation_rules || defaultRules

  // 5. üîó Assemblage final avec structure technique
  enhancedPrompt += `

INSTRUCTIONS TECHNIQUES POUR LA R√âPONSE:
Votre r√©ponse doit √™tre structur√©e en deux parties:

1. ANALYSE TEXTUELLE: Suivez exactement les instructions du prompt syst√®me ci-dessus.
   STRUCTUREZ votre analyse avec des sections num√©rot√©es (1., 2., 3., etc.) pour chaque zone/r√©gion identifi√©e.

2. ANNOTATIONS: Pour chaque section num√©rot√©e de votre analyse, cr√©ez UNE annotation correspondante:
{
  "annotations": [
    {
      "id": "zone_1",
      "type": "info",
      "title": "Nom de la zone (ex: Navigation Lat√©rale, En-t√™te, etc.)",
      "description": "STRUCTUREZ le contenu avec ce format exact: ${formattingInstructions}",
      "x": [position X en pourcentage 0-100],
      "y": [position Y en pourcentage 0-100], 
      "width": [largeur en pourcentage],
      "height": [hauteur en pourcentage],
      "color": "#0066cc"
    }
  ]
}

R√àGLES TECHNIQUES CRUCIALES:
${annotationRules}
- Chaque annotation doit reprendre INT√âGRALEMENT le contenu de la section correspondante
- Incluez TOUS les d√©tails selon le format sp√©cifi√©
- Ne r√©sumez pas, ne raccourcissez pas : copiez TOUT le texte de chaque section

S√©parez les deux parties par "---ANNOTATIONS---"`

  return enhancedPrompt
}
```

#### **Analyse OpenAI (GPT-4o)**

```typescript
const response = await openai.chat.completions.create({
  model: "gpt-4o",
  messages: [
    { role: "system", content: enhancedPrompt },
    { 
      role: "user", 
      content: [
        { type: "text", text: "Analysez cette image..." },
        { type: "image_url", image_url: { url: imageUrl } }
      ]
    }
  ],
  max_tokens: 4000,
  temperature: 0.1
})
```

#### **Analyse Anthropic (Claude 3.5)**

```typescript
const response = await anthropic.messages.create({
  model: "claude-3-5-sonnet-latest",
  max_tokens: 4000,
  system: enhancedPrompt,
  messages: [{
    role: "user",
    content: [
      { type: "text", text: "Analysez cette image..." },
      { 
        type: "image", 
        source: { 
          type: "base64", 
          media_type: "image/jpeg",
          data: imageBase64 
        }
      }
    ]
  }]
})
```

### 5. üìä **Traitement des R√©sultats**

```typescript
// Parsing de la r√©ponse IA
const parts = fullResponse.split('---ANNOTATIONS---')
const textualAnalysis = parts[0].trim()
const annotationsData = JSON.parse(parts[1].trim())

// Structure des annotations
const annotations = [
  {
    id: "zone_1",
    type: "info", // ou "issue", "recommendation"
    title: "Navigation Lat√©rale",
    description: "Analyse d√©taill√©e...",
    x: 10,      // Position X en %
    y: 15,      // Position Y en %
    width: 25,  // Largeur en %
    height: 80, // Hauteur en %
    color: "#0066cc"
  }
  // ...autres annotations
]
```

### 6. üé® **Affichage des R√©sultats**

```typescript
// Composant ImageAnnotationViewer
<ImageAnnotationViewer
  imageFile={uploadedImageFile}
  analysisResults={analysisResults.map(result => ({
    type: analysisType.name,
    issues: annotations.filter(ann => ann.type === 'issue').length,
    recommendations: annotations.filter(ann => ann.type === 'recommendation').length,
    annotations: result.result.annotations,
    summary: result.result.content
  }))}
  isAnalyzing={false}
/>
```

## üîÑ **√âtats du Workflow**

### **√âtats de l'Interface**

1. **`upload`** - Interface d'upload d'image
2. **`configure`** - S√©lection types d'analyse + provider
3. **`analyze`** - Analyses en cours
4. **`results`** - Affichage des r√©sultats avec annotations

### **√âtats des Analyses**

1. **`processing`** - Analyse en cours
2. **`completed`** - Analyse termin√©e avec succ√®s
3. **`error`** - Analyse √©chou√©e

## üóÑÔ∏è **Base de Donn√©es**

### **Tables Principales**

```sql
-- Images upload√©es
images: id, user_id, original_name, storage_path, public_url, 
        file_hash, metadata, status, uploaded_at

-- Types d'analyses configur√©s
analysis_types: id, name, system_prompt, coordination_prompt,
                formatting_instructions, annotation_rules

-- R√©sultats d'analyses
analyses: id, user_id, image_id, analysis_type_id, provider,
          ai_model, result_json, duration_ms, status, created_at

-- Cl√©s API utilisateurs
api_keys: id, user_id, provider, encrypted_key, active, last_used
```

## üîß **Gestion d'Erreurs**

### **Points de Contr√¥le**

1. **Authentification** ‚Üí Redirection login
2. **Cl√©s API** ‚Üí Message configuration
3. **Validation fichier** ‚Üí Erreur format/taille
4. **Upload Supabase** ‚Üí Erreur stockage
5. **Appel IA** ‚Üí Erreur provider/quota
6. **Parsing r√©sultats** ‚Üí Erreur format r√©ponse

### **R√©cup√©ration d'Erreurs**

```typescript
try {
  // Analyse IA
  const result = await analyzeWithProvider(...)
} catch (error) {
  // Marquer l'analyse comme √©chou√©e
  await supabase.from('analyses').update({
    status: 'error',
    result_json: { error: error.message }
  }).eq('id', analysisId)
  
  // Continuer avec les autres analyses
  continue
}
```

## üöÄ **Performance & Optimisations**

### **Optimisations Impl√©ment√©es**

1. **D√©duplication** - Hash des images pour √©viter les doublons
2. **Streaming** - R√©ponses en temps r√©el pour les analyses longues
3. **Cache** - Mise en cache des types d'analyses
4. **Compression** - Images optimis√©es pour l'IA
5. **Parall√©lisation** - Analyses multiples en s√©quence

### **Limites Techniques**

- **Vercel Edge Functions** : 30s timeout max
- **OpenAI** : 4000 tokens max par r√©ponse
- **Claude** : 20MB max par image
- **Supabase Storage** : 50MB max par fichier

## üì± **Interface Utilisateur**

### **Composants Cl√©s**

1. **`real-analyze/page.tsx`** - Page principale
2. **`AnalysisChat`** - Chat IA + s√©lection provider
3. **`ImageAnnotationViewer`** - Visualisation annotations
4. **`ImageUploader`** - Upload d'images

### **Synchronisation d'√âtat**

```typescript
// √âtat partag√© entre composants
const [selectedProvider, setSelectedProvider] = useState('openai')
const [analysisResults, setAnalysisResults] = useState([])
const [isAnalyzing, setIsAnalyzing] = useState(false)

// Props synchronis√©es
<AnalysisChat 
  selectedProvider={selectedProvider}
  onProviderChange={setSelectedProvider}
  analysisMessages={analysisMessages}
  onStartAnalysis={handleStartAnalyses}
/>
```

## üéØ **Workflow Complet - Exemple**

1. **Utilisateur** acc√®de √† `/real-analyze`
2. **Upload** d'une capture d'√©cran d'interface web
3. **S√©lection** "Analyse d'Accessibilit√©" + "Claude"
4. **Analyse** ‚Üí Claude identifie 5 zones avec probl√®mes d'accessibilit√©
5. **R√©sultats** ‚Üí Annotations visuelles + recommandations d√©taill√©es
6. **Chat** ‚Üí Questions/r√©ponses avec Claude sur les am√©liorations
7. **Nouvelle analyse** ‚Üí Possibilit√© d'analyser la m√™me image diff√©remment

---

*Ce workflow garantit une exp√©rience utilisateur fluide et des analyses IA de haute qualit√© pour l'accessibilit√© web.*
